This section describes the information on how to manage Spark applications in a Kubernetes cluster submitted to the Kubeflow Spark Operator.

# Table of Contents

* [Running Applications](#running-applications)
  * [Manual Deployment](#manual-deployment)
  * [Airflow DAG](#airflow-dag)
  * [TLS Enabled for Application](#tls-enabled-for-application)
* [Running Applications on Restricted Environment](#running-applications-on-restricted-environment)
* [Stopping Applications](#stopping-applications)
* [Upgrading Applications](#upgrading-applications)
* [Applications Auto-Recovery Using Checkpoints](#applications-auto-recovery-using-checkpoints)
* [Applications High Availability](#applications-high-availability)
* [Environment Variables](#environment-variables)
* [Python Applications](#python-applications)
* [S3 Storage](#s3-storage)
* [Enabling Authentication Using OAuth2 Proxy](#enabling-authentication-using-oauth2-proxy)

# Running Applications

After the Kubeflow Spark Operator has been installed, the Spark applications can be submitted to a Kubernetes cluster.  
A custom resource of the `SparkApplication` type should be submitted to the Kubernetes cluster in order to run Spark applications by the Spark Operator.  
For more information on how to prepare a CR file of the Spark application, refer to the [Spark Application on Kubernetes](./user-guide.md#spark-application-on-kubernetes) section.

The possible ways to submit application CR files to the Kubernetes cluster are described below.

## Manual Deployment

The following steps describe how to submit an example of the Spark-Pi application provided by an Apache Spark:

1. Download the [spark-pi.yaml](../../docs/public/examples/app-crs/spark-pi.yaml) file.
1. Edit the **spark-pi.yaml** file by setting the correct values for the following:
    * `metadata.namespace` - The namespace, where the applications can be deployed, as described in the [Prerequisites](/docs/installation-guide.md#prerequisites) section of the _Spark Operator Installation Procedure_.
    * `spec.driver.serviceAccount` - The service account for the Spark applications. See the [NOTE ServiceAccount](/docs/user-guide.md#note-serviceaccount) section for the serviceAccount name details.
1. Deploy the application to Kubernetes.

   ```
   kubectl apply -f <path to spark-pi.yaml> --namespace <namespace to install the application>
   #Example
   kubectl apply -f spark-pi.yaml --namespace spark-apps
   ```

Alternatively, it is possible to submit the applications using Kubernetes dashboard or openshift UI.

## Airflow DAG

An Airflow DAG can be used to submit a Spark application to the Kubernetes cluster.  

![Airflow DAG to submit Spark apps](/docs/public/images/airflow_spark_streaming_dag_graph_view.png)

* Airflow provides [airflow.providers.cncf.kubernetes.operators.spark_kubernetes](https://airflow.apache.org/docs/apache-airflow-providers-cncf-kubernetes/stable/_api/airflow/providers/cncf/kubernetes/operators/spark_kubernetes/index.html). It can be used to submit Spark applications to the Spark Operator in Kubernetes.  
  The task that submits an application completes after the submission.
  
* In order to monitor the submitted Spark application, [airflow.providers.cncf.kubernetes.sensors.spark_kubernetes](https://airflow.apache.org/docs/apache-airflow-providers-cncf-kubernetes/stable/_api/airflow/providers/cncf/kubernetes/sensors/spark_kubernetes/index.html) can be used.  
  The task continuously runs and shows the Spark application status in the logs.

 ![Airflow DAG Sensor Task](/docs/public/images/airflow_spark_streaming_dag_sensor_task.png)

## TLS Enabled for Application

1. To activate TLS in a Spark application, you need to add this section to the yaml file of the application:

```yaml
spec:
  sparkUIOptions:
    ingressTLS:
      - secretName: <secret for this app>
        hosts:
          - <name app + url_ingress_format>
```

Example app:
[spark-pi-event-logs-s3-https.yaml](../../docs/public/examples/app-crs/spark-pi-event-logs-s3-https.yaml)

2. Edit the Spark file of the application by setting the correct value for the following:

`spec.sparkUIOptions.ingressTLS.secretName` - The name of the secret with the created certificate.

`spec.sparkUIOptions.ingressTLS.hosts` - The URL of your application that is generated by the rule, <name app + url_ingress_format>.

# Running Applications on Restricted Environment

The following `securityContext` should be added in the application CR to driver's and executor's configuration to support deployment on Kubernetes with Pod Security Standards Restricted profile enabled.

```yaml
apiVersion: "sparkoperator.k8s.io/v1beta2"
kind: SparkApplication
metadata:
 name: spark-pi
 namespace: spark-apps
spec:
 type: Scala
 mode: cluster
 image: "ghcr.io/netcracker/qubership-spark-customized:main"
 imagePullPolicy: Always
 mainClass: org.apache.spark.examples.SparkPi
 mainApplicationFile: "local:///opt/spark/examples/jars/spark-examples_2.12-3.4.1.jar"
 sparkVersion: "3.5.5"
 sparkConf:
  "spark.kubernetes.submission.connectionTimeout": "60000000"
  "spark.kubernetes.submission.requestTimeout": "60000000"
  "spark.kubernetes.driver.connectionTimeout": "60000000"
  "spark.kubernetes.driver.requestTimeout": "60000000"
 arguments:
  - "100000"
 restartPolicy:
  type: Never
 driver:
  cores: 1
  coreLimit: "1200m"
  memory: "512m"
  labels:
   version: 3.5.5
  serviceAccount: sparkoperator-spark
  securityContext:
   seccompProfile:
    type: RuntimeDefault
   allowPrivilegeEscalation: false
   runAsNonRoot: true
   capabilities:
    drop:
     - ALL
 executor:
  cores: 1
  instances: 1
  memory: "512m"
  labels:
   version: 3.5.5
  securityContext:
   seccompProfile:
    type: RuntimeDefault
   allowPrivilegeEscalation: false
   runAsNonRoot: true
   capabilities:
    drop:
     - ALL
```

# Stopping Applications

When a Spark application CR file is submitted to the Kubernetes cluster, the Spark Operator creates a driver pod of the application, then executor pods are created and managed by the driver pod.  
If executors fail, the driver creates new ones. If the driver pod fails, the Spark Operator initiates a new driver creation if only an appropriate restart policy is configured for the application. For the restart policy details, refer to the [Configuring Automatic Application Restart and Failure Handling](https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/master/docs/user-guide.md#configuring-automatic-application-restart-and-failure-handling) section.

The driver should be stopped in order to stop a Spark application. It can be done by either of the following:

* By deleting the driver pod. Note that the restart policy is configured and Spark Operator restarts the application.
  
  ```
  kubectl delete pod <spark_app_name>-driver --namespace <namespace of Spark application>
  ```

* By deleting the application CR file. The restart policy does not affect in this case.
  
  ```
  kubectl delete sparkapplications.sparkoperator.k8s.io <spark_app_CR_name> --namespace <namespace of Spark application>
  ```

# Upgrading Applications

The application's CR file submitted to the Kubernetes cluster should be updated in order to upgrade a Spark application.  
If the changes are made in the application source code, a new docker image of the application should be built and updated in the CR file.  
The updated CR file can then be resubmitted to the Kubernetes.

```
   kubectl apply -f <path to spark app .yaml> --namespace <namespace to install the application>
```

In case of using `Airflow DAG` for Spark applications submission, the old application should be deleted before submitting an updated CR file.  
Currently, an Airflow task based on [airflow.providers.cncf.kubernetes.operators.spark_kubernetes](https://airflow.apache.org/docs/apache-airflow-providers-cncf-kubernetes/stable/_api/airflow/providers/cncf/kubernetes/operators/spark_kubernetes/index.html) fails while submitting an application which already exists.  

There is an open issue in Airflow github community related to this - [Allow deleting existing spark application before creating new one via SparkKubernetesOperator in Kubernetes](https://github.com/apache/airflow/issues/16290).

# Applications Auto-Recovery Using Checkpoints

For stateful streaming applications, Spark provides a `Checkpointing` mechanism to recover the application after a failure. This allows the application to periodically save its state and use it to recover. The S3 storage can be used to store the checkpoints.    
For more information, refer to the _Official Spark Documentation_ at [https://spark.apache.org/docs/3.5.5/streaming-programming-guide.html#checkpointing](https://spark.apache.org/docs/3.5.5/streaming-programming-guide.html#checkpointing).

The Spark Operator's `Restart Policies` can be used to automate the recovery process.  
For more information about configuring automatic application restart and failure handling, refer to [https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/master/docs/user-guide.md#configuring-automatic-application-restart-and-failure-handling](https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/master/docs/user-guide.md#configuring-automatic-application-restart-and-failure-handling).

To summarize, checkpointing should be configured in the source code of a stateful streaming application, and a restart policy should be configured in the application's CR file.

The Spark official documentation recommends using fault-tolerant file systems like HDFS or S3 to store checkpoints.  
For now, the Spark Operator does not support Kerberos. This makes it impossible to use a Kerberized HDFS as a checkpoint storage. For more details, refer to the open issue in the community at [https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/issues/302](https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/issues/302).

Therefore, the possible storage options are:

* S3 - Refer to [S3 Storage](#s3-storage) for configuration details.
* Kubernetes Persistence volumes. Not recommended by Spark official documentation.   
  Note that the volume should support access from any node, because when an application fails, it can be restarted on any node of the Kubernetes cluster.

Refer to the [spark-streaming-checkpoint sample app](../../spark-sample-apps/spark-streaming-checkpoint) for application examples.

# Applications High Availability 

There are no native Spark mechanisms to support the applications' high availability.  
The Spark Operator supports applications' auto-restarting in case of a failure. This is configured in an application's CR file. Refer to [Configuring Automatic Application Restart and Failure Handling](https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/master/docs/user-guide.md#configuring-automatic-application-restart-and-failure-handling) for details.

The Operator itself can be deployed in the HA mode. For details, refer to the [High Available Deployment](/docs/installation-guide.md#high-available-deployment) section in the _Spark Operator Installation Procedure_.

# Environment Variables

Refer to the following documentation:
* [Specifying Environment Variables](https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/master/docs/user-guide.md#specifying-environment-variables)
* [Using Secrets As Environment Variables](https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/master/docs/user-guide.md#using-secrets-as-environment-variables)
* [Environment Variables in 3.5.5](/docs/public/user-guide.md#environment-variables-in-332)

# Python Applications

It is also possible to run Spark applications with the Spark Operator. For applications, it is necessary to use the [spark python image](/spark-customized/py). This image also includes python3.11 and pyspark 3.5.5. `spark-py-pi` application. An example can be found [here](/examples/spark-py-pi.yaml).

# S3 Storage

Spark applications can use S3 storage to store the event logs, checkpointing, to read files, and to output files.

Two additional libs are required for Spark applications to work with S3:

* `aws-java-sdk-bundle-xxx.jar` 
* `hadoop-aws-xxx.jar` 

These jars are already included in the Qubership Spark Images release.

To connect to S3, the application should be properly configured:

1. Add a Kubernetes secret with base64 encoded S3 credentials to the Spark applications namespace.

   ```yaml
   apiVersion: v1
   kind: Secret
   metadata:
   name: s3-cred
   type: Opaque
   data:
   ## Access Key for MinIO, base64 encoded (echo -n 'minioaccesskey' | base64)
   accesskey: bWluaW9hY2Nlc3NrZXk=
   ## Secret Key for MinIO, base64 encoded (echo -n 'miniosecretkey' | base64)
   secretkey: bWluaW9zZWNyZXRrZXk=
   ```
   
1. Add S3 credentials env variables for driver and executors referring to the added secret.

   ```yaml
    driver:
      ...
      env:
        - name: AWS_ACCESS_KEY_ID
          valueFrom:
            secretKeyRef:
              name: s3-cred
              key: accesskey
        - name: AWS_SECRET_ACCESS_KEY
          valueFrom:
            secretKeyRef:
              name: s3-cred
              key: secretkey
    executor:
      ...
      env:
        - name: AWS_ACCESS_KEY_ID
          valueFrom:
            secretKeyRef:
              name: s3-cred
              key: accesskey
        - name: AWS_SECRET_ACCESS_KEY
          valueFrom:
            secretKeyRef:
              name: s3-cred
              key: secretkey
   ```
   
1. Specify the following properties in the applications CR file.

   ```yaml
   spec:
     hadoopConf:
       "fs.s3a.endpoint": <endpoint to S3 storage> # mandatory
       "fs.s3a.impl": org.apache.hadoop.fs.s3a.S3AFileSystem # mandatory
       "fs.s3a.connection.ssl.enabled": "false"
       "fs.s3a.path.style.access": "true" # mandatory
       "fs.s3a.committer.magic.enabled": "false"
       "fs.s3a.committer.name": directory # mandatory
       "fs.s3a.committer.staging.abort.pending.uploads": "true"
       "fs.s3a.committer.staging.conflict-mode": append
       "fs.s3a.connection.timeout": "200000"
   ```
   
An example of S3 configured CR can be viewed in [spark-streaming-checkpoint-s3.yaml](../../spark-sample-apps/spark-streaming-checkpoint/cr/spark-streaming-checkpoint-s3.yaml).

# Enabling Authentication Using OAuth2 Proxy

Authentication for the Spark application UI can be configured using the third-party OAuth2 Proxy service. It allows to add authentication
without changing the source code of the application. The current implementation of OAuth2 Proxy does not allow to configure authentication for multiple hosts. 
Therefore, a separate instance of OAuth2 Proxy should be deployed for each Spark Application.
Applications ingress is configured to check a request's authentication and redirect a user to the authentication page.
Keycloak is used as the Identity Provider.

To enable OAuth2 Proxy authentication for Spark applications:

1. Deploy Keycloak.
2. Configure Keycloak by referring to the official OAuth2 Proxy documentation at [https://oauth2-proxy.github.io/oauth2-proxy/docs/configuration/oauth_provider#keycloak-oidc-auth-provider](https://oauth2-proxy.github.io/oauth2-proxy/docs/configuration/oauth_provider#keycloak-oidc-auth-provider).
   In general, you need to:
    * Create and configure a client.
    * Add Group Membership, Audience mappers to the client.
    * Create users to authenticate to the application UI.
3. Prepare OAuth2 Proxy deployment.
   Helm chart can be found in the OAuth2 Proxy GitHub repository at [https://github.com/oauth2-proxy/manifests/releases](https://github.com/oauth2-proxy/manifests/releases).  
   OAuth2 Proxy should be configured based on the Spark application and Keycloak parameters.  
   Following is the main configuration required for the OAuth2 Proxy deployment:

   ```yaml
   fullnameOverride: oauth2-proxy-<app_name> # to relate Kubernetes objects, such as service, configmap etc., to the application
   config:
     # OAuth client ID
     clientID: "<your_client_id>" # set your client's ID
     # OAuth client secret
     clientSecret: "<your_client_secret>" # set your client's secret
     configFile: |-
       email_domains=[ "*" ]
       insecure_oidc_allow_unverified_email=true
       oidc_issuer_url="http://<keycloak_address>/auth/realms/<keycloak_realm_name>"
       login_url="http://<keycloak_address>/auth/realms/<keycloak_realm_name>"
       cookie_secure=false
       provider="keycloak-oidc"
       code_challenge_method="S256"
       session_cookie_minimal=true # to avoid 502 error response due to buffer size, or set nginx.ingress.kubernetes.io/proxy-buffer-size: 32k on ingress
       upstreams="http://<spark_app_service_address>:<app_port>" # port is 4040 by default
   ingress:
     enabled: true
     path: /
     supportsPathType: true
     pathType: ImplementationSpecific
     hosts:
       - <specify_oauth2_proxy_ingress_host>
   ```

4. Add annotations to the application's ingress to require authentication in the CR file of the application. The CR file of the application can be added to the OAuth2 Proxy helm chart.

```yaml
apiVersion: "sparkoperator.k8s.io/v1beta2"
kind: SparkApplication
...
spec:
  ...
  sparkUIOptions:
    ingressAnnotations:
      nginx.ingress.kubernetes.io/auth-url: "https://<oauth2_proxy_address>/oauth2/auth"
      nginx.ingress.kubernetes.io/auth-signin: "https://<oauth2_proxy_address>/oauth2/start?rd=$scheme://$best_http_host$request_uri"
      nginx.ingress.kubernetes.io/auth-response-headers: "x-auth-request-user, x-auth-request-email, x-auth-request-access-token"
      # nginx.ingress.kubernetes.io/proxy-buffer-size: 32k # set in case of getting 502 error code
```

5. Deploy the Helm chart containing OAuth2 Proxy and the application CR file.
6. After the app is started, proceed to the application's ingress. It should require authentication. 
