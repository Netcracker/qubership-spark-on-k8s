# Default values for spark-history-server.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.


replicas: 1


image:
  repository: ghcr.io/netcracker/qubership-spark-customized
  pullPolicy: IfNotPresent
  tag: main

imagePullSecrets: []
nameOverride: ""
fullnameOverride: "spark-history-server"

priorityClassName: ~

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

podAnnotations: {}

affinity: {}

podSecurityContext: {}
  # fsGroup: 2000

securityContext: {}

service:
  externalPort: 80
  internalPort: 18080
  type: ClusterIP

ingress:
  enabled: true
  host: spark-history-server.your.cloud.qubership.org
  path: /
    # kubernetes.io/ingress.class: nginx
  # nginx.ingress.kubernetes.io/force-ssl-redirect: 'true'
  # host: 'spark-history-server.your.cloud.qubership.org'
  # path: '/'
  tls:
    # Enable TLS termination for the web Ingress
    enabled: false
    # the name of a pre-created Secret containing a TLS private key and certificate
    secretName: ""

# Enable cert-manager integration
certManagerInegration:
  enabled: false
  secretName: spark-history-server-services-tls-certificate
  duration: 365
  subjectAlternativeName:
    additionalDnsNames: [ ]
    additionalIpAddresses: [ ]
  clusterIssuerName: ~


#resources: {}
resources:
  limits:
    cpu: 300m
    memory: 512Mi
  requests:
    cpu: 200m
    memory: 256Mi

livenessProbe:
    enabled: true
    initialDelaySeconds: 10
    timeoutSeconds: 20
    failureThreshold: 5
    periodSeconds: 60

readinessProbe:
    enabled: true
    initialDelaySeconds: 15
    timeoutSeconds: 5
    failureThreshold: 5
    periodSeconds: 30
    
hostAliases: []

kerberos:
  enabled: false
  principal: ""
  keytab: ""
  config: ""

sparkProperties: ""
#sparkProperties: |
#  spark.history.fs.eventLog.rolling.maxFilesToRetain=5
#  spark.hadoop.fs.s3a.connection.ssl.enabled=false
#  spark.hadoop.fs.s3a.committer.magic.enabled=false

initJobCredentialsSecret:
  annotations:
    "helm.sh/hook": pre-install, pre-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded,hook-failed
    "helm.sh/hook-weight": "-15"

s3InitJob:
  enabled: true
  awsSigV4: "aws:minio:s3:s3"
  priorityClassName: ~
  initAnnotations:
    "helm.sh/hook": pre-install, pre-upgrade
    "helm.sh/hook-weight": "-10"
    "helm.sh/hook-delete-policy": before-hook-creation
  resources:
    limits:
      cpu: 50m
      memory: 64Mi
    requests:
      cpu: 50m
      memory: 64Mi

logDirectory: 's3a://tmp/spark/logs'
#logDirectory: 'hdfs://datalake/spark-kuber/logs/'

s3:
  enabled: true
  endpoint: http://minioaddress.your.cloud.qubership.org
  accesskey: yourminioaccesskey
  secretkey: yourminiosecretkey

hdfs:
  enabled: false
  hdfs_site: ""
  core_site: ""

oauth2Proxy:
  enabled: false
      

  image:
    repository: "quay.io/oauth2-proxy/oauth2-proxy"
    tag: "v7.13.0"
    pullPolicy: "IfNotPresent"
    command: []

  # Force the target Kubernetes version (it uses Helm `.Capabilities` if not set).
  # This is especially useful for `helm template` as capabilities are always empty
  # due to the fact that it doesn't query an actual cluster
  kubeVersion:

  # Oauth client configuration specifics
  config:
    # Add config annotations
    annotations: {}
    # OAuth client ID
    clientID: ""
    # OAuth client secret
    #clientSecret: "Hk6i3WeUSoRFZpBSXAJU1yZItOpBMwHb"
    # platform
    #IDP
    clientSecret: ""
    #OCS
    #clientSecret: "Hk6i3WeUSoRFZpBSXAJU1yZItOpBMwHb"
    # Create a new secret with the following command
    # openssl rand -base64 32 | head -c 32 | base64
    # Use an existing secret for OAuth2 credentials (see secret.yaml for required fields)
    # Example:
    # existingSecret: secret
    cookieSecret: "XXXXXXXXXXXXXXXX"
    # The name of the cookie that oauth2-proxy will create
    # If left empty, it will default to the release name
    cookieName: ""
    google: {}
    configFile: "" # |-
#      email_domains=[ "*" ]
#      insecure_oidc_allow_unverified_email=true
#      oidc_issuer_url="http://public-gateway-keycloak.dashboard.cloud.qubership.com/auth/realms/yourrealm"
#      login_url="http://public-gateway-keycloak.dashboard.cloud.qubership.com/auth/realms/yourrealm"
#      cookie_secure=false
#      provider="keycloak-oidc"
#      code_challenge_method="S256"
#      session_cookie_minimal=true
#      upstreams="http://spark-history-server.spark-operator-gcp.svc:80"
    # Custom configuration file: oauth2_proxy.cfg
    # configFile: |-
    #   pass_basic_auth = false
    #   pass_access_token = true
    # Use an existing config map (see configmap.yaml for required fields)
    # Example:
    # existingConfig: config

  alphaConfig:
    enabled: false
    # Add config annotations
    annotations: {}
    # Arbitrary configuration data to append to the server section
    serverConfigData: {}
    # Arbitrary configuration data to append to the metrics section
    metricsConfigData: {}
    # Arbitrary configuration data to append
    configData: {}
    # Arbitrary configuration to append
    # This is treated as a Go template and rendered with the root context
    configFile: ""
    # Use an existing config map (see secret-alpha.yaml for required fields)
    existingConfig: ~
    # Use an existing secret
    existingSecret: ~

  # Optionally specify an array of imagePullSecrets.
  # Secrets must be manually created in the namespace.
  # ref: https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod
  # imagePullSecrets:
    # - name: myRegistryKeySecretName

  # Set a custom containerPort if required.
  # This will default to 4180 if this value is not set and the httpScheme set to http
  # This will default to 4443 if this value is not set and the httpScheme set to https
  # containerPort: 4180

  extraArgs: {}
  extraEnv: []

  # -- Custom labels to add into metadata
  customLabels: {}

  # To authorize individual email addresses
  # That is part of extraArgs but since this needs special treatment we need to do a separate section
  authenticatedEmailsFile:
    enabled: false
    # Defines how the email addresses file will be projected, via a configmap or secret
    persistence: configmap
    # template is the name of the configmap what contains the email user list but has been configured without this chart.
    # It's a simpler way to maintain only one configmap (user list) instead changing it for each oauth2-proxy service.
    # Be aware the value name in the extern config map in data needs to be named to "restricted_user_access" or to the
    # provided value in restrictedUserAccessKey field.
    template: ""
    # The configmap/secret key under which the list of email access is stored
    # Defaults to "restricted_user_access" if not filled-in, but can be overridden to allow flexibility
    restrictedUserAccessKey: ""
    # One email per line
    # example:
    # restricted_access: |-
    #   name1@domain
    #   name2@domain
    # If you override the config with restricted_access it will configure a user list within this chart what takes care of the
    # config map resource.
    restricted_access: ""
    annotations: {}
    # helm.sh/resource-policy: keep

  service:
    type: ClusterIP
    # when service.type is ClusterIP ...
    # clusterIP: 192.0.2.20
    # when service.type is LoadBalancer ...
    # loadBalancerIP: 198.51.100.40
    # loadBalancerSourceRanges: 203.0.113.0/24
    # when service.type is NodePort ...
    # nodePort: 80
    portNumber: 4180
    portName: http-oauth2
    # Protocol set on the service
    appProtocol: http
    annotations: {}
    # foo.io/bar: "true"
    # configure externalTrafficPolicy
    externalTrafficPolicy: ""
    # configure internalTrafficPolicy
    internalTrafficPolicy: ""
    # configure service target port
    targetPort: ""
    # Configures the service to use IPv4/IPv6 dual-stack.
    # Ref: https://kubernetes.io/docs/concepts/services-networking/dual-stack/
    ipDualStack:
      enabled: false
      ipFamilies: ["IPv6", "IPv4"]
      ipFamilyPolicy: "PreferDualStack"
    # Configure traffic distribution for the service
    # Ref: https://kubernetes.io/docs/concepts/services-networking/service/#traffic-distribution
    trafficDistribution: ""
  ## Create or use ServiceAccount
  ## Create or use ServiceAccount
  serviceAccount:
    ## Specifies whether a ServiceAccount should be created
    enabled: true
    ## The name of the ServiceAccount to use.
    ## If not set and create is true, a name is generated using the fullname template
    name: spark-history-auth
    automountServiceAccountToken: true
    annotations: {}
    ## imagePullSecrets for the service account
    imagePullSecrets: []
    # - name: myRegistryKeySecretName
  # Network policy settings.
  networkPolicy:
    create: false
    ingress: []
    egress: []  

  fullnameOverride: oauth2-proxy-history

  ingress:
    enabled: true
    # className: nginx
    path: /
    # Only used if API capabilities (networking.k8s.io/v1) allow it
    supportsPathType: true
    pathType: ImplementationSpecific
    hosts:
#    - spark-history-auth-proxy.dashboard.cloud.qubership.com
    #- auth-proxy-oauth2-proxy.dashboard.cloud.qubership.com
#    extraPaths:
#    - path: /*
#      pathType: ImplementationSpecific
#      backend:
#        service:
#          #name: auth-proxy-oauth2-proxy
#          name: oauth2-proxy-spark-history
#          port:
#            name: use-annotation
    # Used to create an Ingress record.
    # hosts:
      # - chart-example.local
    # Extra paths to prepend to every host configuration. This is useful when working with annotation based services.
    # Warning! The configuration is dependant on your current k8s API version capabilities (networking.k8s.io/v1)
    # extraPaths:
    # - path: /*
    #   pathType: ImplementationSpecific
    #   backend:
    #     service:
    #       name: ssl-redirect
    #       port:
    #         name: use-annotation
    labels: {}
    # annotations:
    #   kubernetes.io/ingress.class: nginx
    #   kubernetes.io/tls-acme: "true"
    # tls:
      # Secrets must be manually created in the namespace.
      # - secretName: chart-example-tls
      #   hosts:
      #     - chart-example.local

  resources: {}
    # limits:
    #   cpu: 100m
    #   memory: 300Mi
    # requests:
    #   cpu: 100m
    #   memory: 300Mi

  # Container resize policy for runtime resource updates
  # Ref: https://kubernetes.io/docs/tasks/configure-pod-container/resize-container-resources/
  resizePolicy: []
    # - resourceName: cpu
    #   restartPolicy: NotRequired
    # - resourceName: memory
    #   restartPolicy: RestartContainer  

  extraVolumes: []
    # - name: ca-bundle-cert
    #   secret:
    #     secretName: <secret-name>

  extraVolumeMounts: []
    # - mountPath: /etc/ssl/certs/
    #   name: ca-bundle-cert

  # Additional containers to be added to the pod.
  extraContainers: []
    #  - name: my-sidecar
    #    image: nginx:latest

  # Additional Init containers to be added to the pod.
  extraInitContainers: []
    #  - name: wait-for-idp
    #    image: my-idp-wait:latest
    #    command:
    #    - sh
    #    - -c
    #    - wait-for-idp.sh  

  priorityClassName: ""

  # Host aliases, useful when working "on premise" where (public) DNS resolver does not know about my hosts.
  hostAlias:
    enabled: false
    # ip: "10.xxx.xxx.xxx"
    # hostname: "auth.example.com"

  # [TopologySpreadConstraints](https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/) configuration.
  # Ref: https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/#scheduling
  # topologySpreadConstraints: []

  # Affinity for pod assignment
  # Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  # affinity: {}
  

  # Tolerations for pod assignment
  # Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  tolerations: []

  # Node labels for pod assignment
  # Ref: https://kubernetes.io/docs/user-guide/node-selection/
  nodeSelector: {}

  # Whether to use secrets instead of environment values for setting up OAUTH2_PROXY variables
  proxyVarsAsSecrets: true

  # Configure Kubernetes liveness and readiness probes.
  # Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/
  # Disable both when deploying with Istio 1.0 mTLS. https://istio.io/help/faq/security/#k8s-health-checks
  livenessProbe:
    enabled: true
    initialDelaySeconds: 0
    timeoutSeconds: 1

  readinessProbe:
    enabled: true
    initialDelaySeconds: 0
    timeoutSeconds: 5
    periodSeconds: 10
    successThreshold: 1

  # Configure Kubernetes security context for container
  # Ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  securityContext: {}

  deploymentAnnotations: {}
  podAnnotations: {}
  podLabels: {}
  replicas: 1
  revisionHistoryLimit: 10
  enableServiceLinks: true

  ## PodDisruptionBudget settings
  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/disruptions/
  podDisruptionBudget:
    enabled: true
    minAvailable: 1
    maxUnavailable: null
    # Policy for when unhealthy pods should be considered for eviction.
    # Valid values are "IfHealthyBudget" and "AlwaysAllow".
    # Ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/#unhealthy-pod-eviction-policy
    unhealthyPodEvictionPolicy: ""

  ## Horizontal Pod Autoscaling
  ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 10
    targetCPUUtilizationPercentage: 80
    # targetMemoryUtilizationPercentage: 80
    annotations: {}
    # Configure HPA behavior policies for scaling if needed
    # Ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#configuring-scaling-behavior
    behavior:
      {}
      # scaleDown:
      #   stabilizationWindowSeconds: 300
      #   policies:
      #   - type: Percent
      #     value: 100
      #     periodSeconds: 15
      #   selectPolicy: Min
      # scaleUp:
      #   stabilizationWindowSeconds: 0
      #   policies:
      #   - type: Percent
      #     value: 100
      #     periodSeconds: 15
      #   - type: Pods
      #     value: 4
      #     periodSeconds: 15
      #   selectPolicy: Max
    

  # Configure Kubernetes security context for pod
  # Ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  podSecurityContext: {}

  # whether to use http or https
  httpScheme: http

  initContainers:
    # if the redis sub-chart is enabled, wait for it to be ready
    # before starting the proxy
    # creates a role binding to get, list, watch, the redis master pod
    # if service account is enabled
    waitForRedis:
      enabled: true
      image:
        repository: "alpine"
        tag: "latest"
        pullPolicy: "IfNotPresent"
      # uses the kubernetes version of the cluster
      # the chart is deployed on, if not set
      kubectlVersion: ""
      securityContext:
        enabled: true
        allowPrivilegeEscalation: false
        capabilities:
          drop:
            - ALL
        readOnlyRootFilesystem: true
        runAsNonRoot: true
        runAsUser: 65534
        runAsGroup: 65534
        seccompProfile:
          type: RuntimeDefault
      timeout: 180
      resources:
        {}
        # limits:
        #   cpu: 100m
        #   memory: 300Mi
        # requests:
        #   cpu: 100m
        #   memory: 300Mi

  # Additionally authenticate against a htpasswd file. Entries must be created with "htpasswd -B" for bcrypt encryption.
  # Alternatively supply an existing secret which contains the required information.
  htpasswdFile:
    enabled: false
    existingSecret: ""
    entries: []
    # One row for each user
    # example:
    # entries:
    #  - testuser:$2y$05$gY6dgXqjuzFhwdhsiFe7seM9q9Tile4Y3E.CBpAZJffkeiLaC21Gy

  # Configure the session storage type, between cookie and redis
  sessionStorage:
    # Can be one of the supported session storage cookie|redis
    type: cookie
    redis:
      # Name of the Kubernetes secret containing the redis & redis sentinel password values (see also `sessionStorage.redis.passwordKey`)
      existingSecret: ""
      # Redis password value. Applicable for all Redis configurations. Taken from redis subchart secret if not set. `sessionStorage.redis.existingSecret` takes precedence
      password: ""
      # Key of the Kubernetes secret data containing the redis password value. If you use the redis sub chart, make sure
      # this password matches the one used in redis.global.redis.password (see below).
      passwordKey: "redis-password"
      # Can be one of standalone|cluster|sentinel
      clientType: "standalone"
      standalone:
        # URL of redis standalone server for redis session storage (e.g. `redis://HOST[:PORT]`). Automatically generated if not set
        connectionUrl: ""
      cluster:
        # List of Redis cluster connection URLs. Array or single string allowed.
        connectionUrls: []
        # - "redis://127.0.0.1:8000"
        # - "redis://127.0.0.1:8001"
      sentinel:
        # Name of the Kubernetes secret containing the redis sentinel password value (see also `sessionStorage.redis.sentinel.passwordKey`). Default: `sessionStorage.redis.existingSecret`
        existingSecret: ""
        # Redis sentinel password. Used only for sentinel connection; any redis node passwords need to use `sessionStorage.redis.password`
        password: ""
        # Key of the Kubernetes secret data containing the redis sentinel password value
        passwordKey: "redis-sentinel-password"
        # Redis sentinel master name
        masterName: ""
        # List of Redis cluster connection URLs. Array or single string allowed.
        connectionUrls: []
        # - "redis://127.0.0.1:8000"
        # - "redis://127.0.0.1:8001"

  # Enables and configure the automatic deployment of the redis subchart
  redis:
    # provision an instance of the redis sub-chart
    enabled: false
    # Redis specific helm chart settings, please see:
    # https://github.com/bitnami/charts/tree/master/bitnami/redis#parameters
    # global:
    #   redis:
    #     password: yourpassword
    # If you install Redis using this sub chart, make sure that the password of the sub chart matches the password
    # you set in sessionStorage.redis.password (see above).
    # redisPort: 6379
    # architecture: standalone

  # Enables apiVersion deprecation checks
  checkDeprecation: true

  metrics:
    # Enable Prometheus metrics endpoint
    enabled: false
    # Serve Prometheus metrics on this port
    port: 44180
    # when service.type is NodePort ...
    # nodePort: 44180
    # Protocol set on the service for the metrics port
    service:
      appProtocol: http
    serviceMonitor:
      # Enable Prometheus Operator ServiceMonitor
      enabled: false
      # Define the namespace where to deploy the ServiceMonitor resource
      namespace: ""
      # Prometheus Instance definition
      prometheusInstance: default
      # Prometheus scrape interval
      interval: 60s
      # Prometheus scrape timeout
      scrapeTimeout: 30s
      # Add custom labels to the ServiceMonitor resource
      labels: {}

      ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS.
      scheme: ""

      ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS.
      ## Of type: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#tlsconfig
      tlsConfig: {}

      ## bearerTokenFile: Path to bearer token file.
      bearerTokenFile: ""

      ## Used to pass annotations that are used by the Prometheus installed in your cluster to select Service Monitors to work with
      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#prometheusspec
      annotations: {}

      ## Metric relabel configs to apply to samples before ingestion.
      ## [Metric Relabeling](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#metric_relabel_configs)
      metricRelabelings: []
      # - action: keep
      #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
      #   sourceLabels: [__name__]

      ## Relabel configs to apply to samples before ingestion.
      ## [Relabeling](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#relabel_config)
      relabelings: []
      # - sourceLabels: [__meta_kubernetes_pod_node_name]
      #   separator: ;
      #   regex: ^(.*)$
      #   targetLabel: nodename
      #   replacement: $1
      #   action: replace


  # Extra K8s manifests to deploy
  extraObjects: []
    # - apiVersion: secrets-store.csi.x-k8s.io/v1
    #   kind: SecretProviderClass
    #   metadata:
    #     name: oauth2-proxy-secrets-store
    #   spec:
    #     provider: aws
    #     parameters:
    #       objects: |
    #         - objectName: "oauth2-proxy"
    #           objectType: "secretsmanager"
    #           jmesPath:
    #               - path: "client_id"
    #                 objectAlias: "client-id"
    #               - path: "client_secret"
    #                 objectAlias: "client-secret"
    #               - path: "cookie_secret"
    #                 objectAlias: "cookie-secret"
    #     secretObjects:
    #     - data:
    #       - key: client-id
    #         objectName: client-id
    #         - key: client-secret
    #           objectName: client-secret
    #         - key: cookie-secret
    #         objectName: cookie-secret
    #       secretName: oauth2-proxy-secrets-store
    #       type: Opaque
